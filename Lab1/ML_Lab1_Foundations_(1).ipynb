{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmPSFd-Wx-fw"
      },
      "source": [
        "# üß† Machine Learning - Lab 1\n",
        "## Foundations: ML Lifecycle, Environment Setup & Data Preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "**Course:** CS4082 Machine Learning  \n",
        "**Institution:** Effat University, Computer Science Department  \n",
        "**Instructor:** Dr. Naila Marir\n",
        "\n",
        "---\n",
        "\n",
        "### üìã Lab Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "1. **Understand** the complete Machine Learning project lifecycle\n",
        "2. **Set up** your development environment using Google Colab and/or VSCode\n",
        "3. **Apply** NumPy and Pandas operations for data manipulation\n",
        "4. **Perform** essential data preprocessing techniques using Pandas\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ppgw6Qhx-fy"
      },
      "source": [
        "# Part 1: Machine Learning Project Lifecycle üîÑ\n",
        "\n",
        "Every successful ML project follows a structured lifecycle. Understanding this lifecycle is crucial before diving into implementation.\n",
        "\n",
        "## 1.1 The Seven Stages of ML Project Lifecycle\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                     MACHINE LEARNING PROJECT LIFECYCLE                       ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                             ‚îÇ\n",
        "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ\n",
        "‚îÇ   ‚îÇ 1. Problem   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 2. Data      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 3. Data      ‚îÇ                 ‚îÇ\n",
        "‚îÇ   ‚îÇ Definition   ‚îÇ    ‚îÇ Collection   ‚îÇ    ‚îÇ Preparation  ‚îÇ                 ‚îÇ\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ\n",
        "‚îÇ          ‚îÇ                                       ‚îÇ                          ‚îÇ\n",
        "‚îÇ          ‚îÇ                                       ‚ñº                          ‚îÇ\n",
        "‚îÇ          ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ\n",
        "‚îÇ          ‚îÇ            ‚îÇ 5. Model     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ 4. Feature   ‚îÇ                 ‚îÇ\n",
        "‚îÇ          ‚îÇ            ‚îÇ Training     ‚îÇ    ‚îÇ Engineering  ‚îÇ                 ‚îÇ\n",
        "‚îÇ          ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ\n",
        "‚îÇ          ‚îÇ                   ‚îÇ                                              ‚îÇ\n",
        "‚îÇ          ‚îÇ                   ‚ñº                                              ‚îÇ\n",
        "‚îÇ          ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ\n",
        "‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 6. Model     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 7. Model     ‚îÇ                 ‚îÇ\n",
        "‚îÇ         (iterate)     ‚îÇ Evaluation   ‚îÇ    ‚îÇ Deployment   ‚îÇ                 ‚îÇ\n",
        "‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ\n",
        "‚îÇ                              ‚îÇ                   ‚îÇ                          ‚îÇ\n",
        "‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ\n",
        "‚îÇ                                 (monitoring & feedback)                     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY483d4Bx-fy"
      },
      "source": [
        "## 1.2 Detailed Breakdown of Each Stage\n",
        "\n",
        "### Stage 1: Problem Definition üéØ\n",
        "\n",
        "**Key Questions to Answer:**\n",
        "- What business/research problem are we trying to solve?\n",
        "- Is ML the right approach for this problem?\n",
        "- What type of ML task is this? (Classification, Regression, Clustering, etc.)\n",
        "- What does success look like? (Define metrics)\n",
        "\n",
        "**Example:**\n",
        "- **Problem:** Predict student performance to enable early intervention\n",
        "- **ML Task Type:** Regression (predicting GPA) or Classification (Pass/Fail)\n",
        "- **Success Metric:** Accuracy > 85%, or RMSE < 0.5\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 2: Data Collection üìä\n",
        "\n",
        "**Key Activities:**\n",
        "- Identify data sources (databases, APIs, files, web scraping)\n",
        "- Collect relevant data\n",
        "- Ensure data quality and quantity\n",
        "- Consider ethical and legal aspects (privacy, consent)\n",
        "\n",
        "**Common Data Sources:**\n",
        "- Kaggle datasets\n",
        "- UCI Machine Learning Repository\n",
        "- Government open data portals\n",
        "- Company databases\n",
        "- Web APIs\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 3: Data Preparation (Preprocessing) üßπ\n",
        "\n",
        "**This is where we spend 60-80% of our time!**\n",
        "\n",
        "**Key Activities:**\n",
        "- Data cleaning (handling missing values, duplicates)\n",
        "- Data transformation (normalization, encoding)\n",
        "- Data integration (merging multiple sources)\n",
        "- Exploratory Data Analysis (EDA)\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 4: Feature Engineering üîß\n",
        "\n",
        "**Key Activities:**\n",
        "- Feature selection (choosing relevant features)\n",
        "- Feature creation (deriving new features)\n",
        "- Feature transformation (log, polynomial)\n",
        "- Dimensionality reduction (PCA, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 5: Model Training üèãÔ∏è\n",
        "\n",
        "**Key Activities:**\n",
        "- Select appropriate algorithms\n",
        "- Split data (train/validation/test)\n",
        "- Train models\n",
        "- Tune hyperparameters\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 6: Model Evaluation üìà\n",
        "\n",
        "**Key Activities:**\n",
        "- Evaluate using appropriate metrics\n",
        "- Compare different models\n",
        "- Analyze errors and failures\n",
        "- Validate on unseen data\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 7: Model Deployment üöÄ\n",
        "\n",
        "**Key Activities:**\n",
        "- Deploy model to production\n",
        "- Monitor performance\n",
        "- Update and retrain as needed\n",
        "- Document and maintain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yw-T9inx-fy"
      },
      "source": [
        "## üí° Exercise 1.1: Problem Definition Practice\n",
        "\n",
        "For each scenario below, identify:\n",
        "1. The ML task type\n",
        "2. Potential input features\n",
        "3. The target variable\n",
        "4. A suitable evaluation metric\n",
        "\n",
        "**Scenarios:**\n",
        "\n",
        "**A)** A hospital wants to predict whether a patient will be readmitted within 30 days.\n",
        "\n",
        "**B)** An e-commerce company wants to group customers based on their shopping behavior.\n",
        "\n",
        "**C)** A real estate company wants to estimate house prices.\n",
        "\n",
        "**Write your answers in the cell below:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnhusMlQx-fz"
      },
      "source": [
        "**Your Answers:**\n",
        "\n",
        "**A) Hospital Readmission:**\n",
        "- ML Task Type: *Binary Classification*\n",
        "- Input Features: *Lab Results, Diagnosis, Patient Details*\n",
        "- Target Variable: *Readmission (Yes/ No)*\n",
        "- Evaluation Metric: *F1-score*\n",
        "\n",
        "**B) Customer Grouping:**\n",
        "- ML Task Type: *Clustering (Unsupervised Learning)*\n",
        "- Input Features: *Customer Information, Customer Purchase Histories, Purchase Frequency*\n",
        "- Target Variable: *Clusters*\n",
        "- Evaluation Metric:\n",
        "\n",
        "**C) House Prices:**\n",
        "- ML Task Type: *Regression*\n",
        "- Input Features: *Number of bedrooms/ bathrooms, Sqr Footage, location(District/ street)*\n",
        "- Target Variable: *House price*\n",
        "- Evaluation Metric: *MAE*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3s0XvCGx-fz"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2: Environment Setup üíª\n",
        "\n",
        "## 2.1 Google Colab\n",
        "\n",
        "Google Colab is a free, cloud-based Jupyter notebook environment that requires no setup.\n",
        "\n",
        "### Advantages:\n",
        "- ‚úÖ No installation required\n",
        "- ‚úÖ Free GPU/TPU access\n",
        "- ‚úÖ Pre-installed ML libraries\n",
        "- ‚úÖ Easy sharing and collaboration\n",
        "- ‚úÖ Google Drive integration\n",
        "\n",
        "### How to Access:\n",
        "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
        "2. Sign in with your Google account\n",
        "3. Create a new notebook or upload an existing one\n",
        "\n",
        "### Useful Colab Shortcuts:\n",
        "| Shortcut | Action |\n",
        "|----------|--------|\n",
        "| `Ctrl + Enter` | Run current cell |\n",
        "| `Shift + Enter` | Run cell and move to next |\n",
        "| `Ctrl + M B` | Insert cell below |\n",
        "| `Ctrl + M A` | Insert cell above |\n",
        "| `Ctrl + M D` | Delete cell |\n",
        "| `Ctrl + M M` | Convert to markdown |\n",
        "| `Ctrl + M Y` | Convert to code |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc29EhMKx-fz",
        "outputId": "f025b0e2-da12-4b09-8812-e2eca0cb3892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab: True\n",
            "üéâ Welcome to Google Colab!\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Colab\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"üéâ Welcome to Google Colab!\")\n",
        "else:\n",
        "    print(\"üìç You're running this locally (VSCode or Jupyter)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo0gKEyKx-f0"
      },
      "outputs": [],
      "source": [
        "# Mounting Google Drive (Colab only)\n",
        "# This allows you to save and access files from your Drive\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqiBWLG_x-f0"
      },
      "source": [
        "## 2.2 VSCode Setup\n",
        "\n",
        "Visual Studio Code is a powerful, customizable code editor.\n",
        "\n",
        "### Installation Steps:\n",
        "\n",
        "1. **Install VSCode:** Download from [code.visualstudio.com](https://code.visualstudio.com)\n",
        "\n",
        "2. **Install Python:** Download from [python.org](https://python.org) (version 3.8+)\n",
        "\n",
        "3. **Install Required Extensions:**\n",
        "   - Python (Microsoft)\n",
        "   - Jupyter (Microsoft)\n",
        "   - Pylance (Microsoft)\n",
        "\n",
        "4. **Create a Virtual Environment:**\n",
        "   ```bash\n",
        "   # In terminal\n",
        "   python -m venv ml_env\n",
        "   \n",
        "   # Activate (Windows)\n",
        "   ml_env\\Scripts\\activate\n",
        "   \n",
        "   # Activate (Mac/Linux)\n",
        "   source ml_env/bin/activate\n",
        "   ```\n",
        "\n",
        "5. **Install Required Libraries:**\n",
        "   ```bash\n",
        "   pip install numpy pandas matplotlib seaborn scikit-learn jupyter\n",
        "   ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSpTQsBwx-f0"
      },
      "outputs": [],
      "source": [
        "# Check installed packages and versions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import sys\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"üîç ENVIRONMENT CHECK\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(f\"NumPy Version: {np.__version__}\")\n",
        "print(f\"Pandas Version: {pd.__version__}\")\n",
        "print(f\"Matplotlib Version: {matplotlib.__version__}\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚úÖ All essential libraries are installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsaK1vByx-f0"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 3: NumPy & Pandas Revision üìö\n",
        "\n",
        "## 3.1 NumPy Essentials\n",
        "\n",
        "NumPy (Numerical Python) is the foundation of scientific computing in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H85sZ2Px-f0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ============================================\n",
        "# CREATING ARRAYS\n",
        "# ============================================\n",
        "\n",
        "# From Python lists\n",
        "arr1 = np.array([1, 2, 3, 4, 5])\n",
        "print(\"1D Array:\", arr1)\n",
        "\n",
        "# 2D Array (Matrix)\n",
        "arr2 = np.array([[1, 2, 3],\n",
        "                 [4, 5, 6]])\n",
        "print(\"\\n2D Array:\")\n",
        "print(arr2)\n",
        "\n",
        "# Special arrays\n",
        "zeros = np.zeros((3, 3))          # 3x3 array of zeros\n",
        "ones = np.ones((2, 4))            # 2x4 array of ones\n",
        "identity = np.eye(3)              # 3x3 identity matrix\n",
        "random_arr = np.random.rand(3, 3) # 3x3 random values [0,1)\n",
        "\n",
        "print(\"\\nZeros Matrix:\")\n",
        "print(zeros)\n",
        "\n",
        "print(\"\\nIdentity Matrix:\")\n",
        "print(identity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0mQseHHx-f0"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ARRAY PROPERTIES\n",
        "# ============================================\n",
        "\n",
        "arr = np.array([[1, 2, 3, 4],\n",
        "                [5, 6, 7, 8],\n",
        "                [9, 10, 11, 12]])\n",
        "\n",
        "print(\"Array:\")\n",
        "print(arr)\n",
        "print(f\"\\nShape: {arr.shape}\")      # (rows, columns)\n",
        "print(f\"Dimensions: {arr.ndim}\")    # Number of dimensions\n",
        "print(f\"Size: {arr.size}\")          # Total elements\n",
        "print(f\"Data Type: {arr.dtype}\")    # Data type of elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzQvT1Rmx-f0"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# INDEXING AND SLICING\n",
        "# ============================================\n",
        "\n",
        "arr = np.array([[1, 2, 3, 4],\n",
        "                [5, 6, 7, 8],\n",
        "                [9, 10, 11, 12]])\n",
        "\n",
        "print(\"Original Array:\")\n",
        "print(arr)\n",
        "\n",
        "# Accessing elements\n",
        "print(f\"\\nElement at [0,0]: {arr[0, 0]}\")    # First element\n",
        "print(f\"Element at [1,2]: {arr[1, 2]}\")      # Row 1, Column 2\n",
        "\n",
        "# Slicing\n",
        "print(f\"\\nFirst row: {arr[0, :]}\")           # All columns of row 0\n",
        "print(f\"First column: {arr[:, 0]}\")          # All rows of column 0\n",
        "print(f\"\\nSubarray [0:2, 1:3]:\")\n",
        "print(arr[0:2, 1:3])                          # Rows 0-1, Columns 1-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qtgG-Rmx-f0"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ARRAY OPERATIONS\n",
        "# ============================================\n",
        "\n",
        "a = np.array([1, 2, 3, 4])\n",
        "b = np.array([5, 6, 7, 8])\n",
        "\n",
        "print(\"a =\", a)\n",
        "print(\"b =\", b)\n",
        "\n",
        "# Element-wise operations\n",
        "print(f\"\\na + b = {a + b}\")\n",
        "print(f\"a * b = {a * b}\")\n",
        "print(f\"a ** 2 = {a ** 2}\")\n",
        "\n",
        "# Statistical operations\n",
        "print(f\"\\nSum of a: {np.sum(a)}\")\n",
        "print(f\"Mean of a: {np.mean(a)}\")\n",
        "print(f\"Std of a: {np.std(a):.2f}\")\n",
        "print(f\"Max of a: {np.max(a)}\")\n",
        "print(f\"Min of a: {np.min(a)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4mdCcc1x-f0"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# RESHAPING ARRAYS\n",
        "# ============================================\n",
        "\n",
        "arr = np.arange(1, 13)  # [1, 2, 3, ..., 12]\n",
        "print(\"Original (1D):\", arr)\n",
        "\n",
        "# Reshape to different dimensions\n",
        "reshaped_3x4 = arr.reshape(3, 4)\n",
        "print(\"\\nReshaped to 3x4:\")\n",
        "print(reshaped_3x4)\n",
        "\n",
        "reshaped_4x3 = arr.reshape(4, 3)\n",
        "print(\"\\nReshaped to 4x3:\")\n",
        "print(reshaped_4x3)\n",
        "\n",
        "# Flatten back to 1D\n",
        "flattened = reshaped_3x4.flatten()\n",
        "print(\"\\nFlattened:\", flattened)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl-vomKmx-f0"
      },
      "source": [
        "## üí° Exercise 3.1: NumPy Practice\n",
        "\n",
        "Complete the following tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18woaWhJx-f0"
      },
      "outputs": [],
      "source": [
        "# Task 1: Create a 5x5 array with values from 1 to 25\n",
        "# Your code here:\n",
        "arr = np.arange(1, 26).reshape(5, 5)\n",
        "print(\"Array (1-25):\")\n",
        "print(arr)\n",
        "\n",
        "\n",
        "# Task 2: Extract the diagonal elements of the array\n",
        "# Hint: use np.diag()\n",
        "# Your code here:\n",
        "diagonal_arr = np.diag(arr)\n",
        "print(\"\\nDiagonal Elements in Array:\")\n",
        "print(diagonal_arr)\n",
        "\n",
        "\n",
        "# Task 3: Calculate the sum of each row\n",
        "# Hint: use np.sum() with axis parameter\n",
        "# Your code here:\n",
        "sums_row = np.sum(arr, axis=1)\n",
        "print(\"\\nSum of Each Row:\")\n",
        "print(sums_row)\n",
        "\n",
        "\n",
        "# Task 4: Normalize the array (subtract mean, divide by std)\n",
        "# Your code here:\n",
        "mean_val = np.mean(arr)\n",
        "std_val = np.std(arr)\n",
        "normalized_array = (arr - mean_val) / std_val\n",
        "print(\"\\nNormalized Array:\")\n",
        "print(normalized_array.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpgQNbUNx-f0"
      },
      "source": [
        "## 3.2 Pandas Essentials\n",
        "\n",
        "Pandas provides high-performance, easy-to-use data structures for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFIz2oc_x-f1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ============================================\n",
        "# SERIES - 1D labeled array\n",
        "# ============================================\n",
        "\n",
        "# Creating a Series\n",
        "grades = pd.Series([85, 90, 78, 92, 88],\n",
        "                   index=['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'])\n",
        "\n",
        "print(\"Student Grades (Series):\")\n",
        "print(grades)\n",
        "\n",
        "# Accessing elements\n",
        "print(f\"\\nAlice's grade: {grades['Alice']}\")\n",
        "print(f\"Mean grade: {grades.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwnmQwFqx-f1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# DATAFRAME - 2D labeled data structure\n",
        "# ============================================\n",
        "\n",
        "# Creating a DataFrame from dictionary\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
        "    'Age': [22, 25, 23, 24, 22],\n",
        "    'Major': ['CS', 'Math', 'CS', 'Physics', 'CS'],\n",
        "    'GPA': [3.8, 3.5, 3.9, 3.7, 3.6]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Student DataFrame:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BPDfpPXx-f1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# EXPLORING DATA\n",
        "# ============================================\n",
        "\n",
        "print(\"Basic Information:\")\n",
        "print(f\"Shape: {df.shape}\")  # (rows, columns)\n",
        "print(f\"\\nColumn names: {list(df.columns)}\")\n",
        "print(f\"\\nData types:\\n{df.dtypes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhIQ_VdHx-f1"
      },
      "outputs": [],
      "source": [
        "# Viewing data\n",
        "print(\"First 3 rows (head):\")\n",
        "print(df.head(3))\n",
        "\n",
        "print(\"\\nLast 2 rows (tail):\")\n",
        "print(df.tail(2))\n",
        "\n",
        "print(\"\\nStatistical summary:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mVaAanvx-f1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# SELECTING DATA\n",
        "# ============================================\n",
        "\n",
        "# Selecting a single column (returns Series)\n",
        "print(\"Names column:\")\n",
        "print(df['Name'])\n",
        "\n",
        "# Selecting multiple columns (returns DataFrame)\n",
        "print(\"\\nName and GPA columns:\")\n",
        "print(df[['Name', 'GPA']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziVX_BwJx-f1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# INDEXING WITH .loc AND .iloc\n",
        "# ============================================\n",
        "\n",
        "# .loc - label-based indexing\n",
        "print(\"Using .loc (label-based):\")\n",
        "print(df.loc[0])              # Row with index 0\n",
        "print()\n",
        "print(df.loc[0:2, 'Name':'Major'])  # Rows 0-2, columns Name to Major\n",
        "\n",
        "# .iloc - integer-based indexing\n",
        "print(\"\\nUsing .iloc (integer-based):\")\n",
        "print(df.iloc[0])             # First row\n",
        "print()\n",
        "print(df.iloc[0:2, 0:2])      # First 2 rows, first 2 columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CbFtUcnx-f1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# FILTERING DATA\n",
        "# ============================================\n",
        "\n",
        "# Boolean filtering\n",
        "print(\"Students with GPA > 3.6:\")\n",
        "print(df[df['GPA'] > 3.6])\n",
        "\n",
        "print(\"\\nCS majors:\")\n",
        "print(df[df['Major'] == 'CS'])\n",
        "\n",
        "# Multiple conditions\n",
        "print(\"\\nCS majors with GPA >= 3.8:\")\n",
        "print(df[(df['Major'] == 'CS') & (df['GPA'] >= 3.8)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xA-m8bBx-f1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ADDING AND MODIFYING COLUMNS\n",
        "# ============================================\n",
        "\n",
        "# Adding a new column\n",
        "df['Scholarship'] = df['GPA'] >= 3.7\n",
        "print(\"With Scholarship column:\")\n",
        "print(df)\n",
        "\n",
        "# Creating a column based on conditions\n",
        "df['Grade_Letter'] = df['GPA'].apply(\n",
        "    lambda x: 'A' if x >= 3.7 else ('B' if x >= 3.3 else 'C')\n",
        ")\n",
        "print(\"\\nWith Grade Letter:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQShUMbSx-f1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# GROUPING AND AGGREGATION\n",
        "# ============================================\n",
        "\n",
        "# Group by Major\n",
        "print(\"Average GPA by Major:\")\n",
        "print(df.groupby('Major')['GPA'].mean())\n",
        "\n",
        "print(\"\\nCount by Major:\")\n",
        "print(df.groupby('Major').size())\n",
        "\n",
        "print(\"\\nMultiple aggregations:\")\n",
        "print(df.groupby('Major').agg({\n",
        "    'GPA': ['mean', 'max', 'min'],\n",
        "    'Age': 'mean'\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOF6ACI2x-f1"
      },
      "source": [
        "## üí° Exercise 3.2: Pandas Practice\n",
        "\n",
        "Complete the following tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC7VgJgpx-f1"
      },
      "outputs": [],
      "source": [
        "# Create this DataFrame first\n",
        "products = pd.DataFrame({\n",
        "    'Product': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse'],\n",
        "    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Accessories', 'Accessories'],\n",
        "    'Price': [1200, 800, 500, 300, 100, 50],\n",
        "    'Stock': [50, 150, 80, 60, 200, 300]\n",
        "})\n",
        "\n",
        "print(\"Products DataFrame:\")\n",
        "print(products)\n",
        "\n",
        "# Task 1: Find products with price > 400\n",
        "# Your code here:\n",
        "high_price = products[products['Price'] > 400]\n",
        "print(\"\\nProducts with Price > 400:\")\n",
        "print(high_price)\n",
        "\n",
        "\n",
        "# Task 2: Calculate total inventory value for each product (Price * Stock)\n",
        "# Add it as a new column called 'Total_Value'\n",
        "# Your code here:\n",
        "products['Total_Value'] = products['Price'] * products['Stock']\n",
        "print(products)\n",
        "\n",
        "\n",
        "# Task 3: Find the average price by category\n",
        "# Your code here:\n",
        "avg_price_by_category = products.groupby('Category')['Price'].mean()\n",
        "print(\"\\nAverage Price by Category:\")\n",
        "print(avg_price_by_category)\n",
        "\n",
        "\n",
        "# Task 4: Sort products by Total_Value in descending order\n",
        "# Your code here:\n",
        "sorted_products = products.sort_values('Total_Value', ascending=False)\n",
        "print(\"\\nProducts Sorted by Total_Value:\")\n",
        "print(sorted_products)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjxXoMNFx-f1"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 4: Data Preprocessing Tutorial üîß\n",
        "\n",
        "Data preprocessing is a critical step in the ML pipeline. We'll cover the essential techniques using a realistic dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1YRMsw9x-f1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create a sample dataset with realistic issues\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples = 100\n",
        "\n",
        "# Generate data with intentional issues for preprocessing practice\n",
        "data = {\n",
        "    'Student_ID': range(1001, 1001 + n_samples),\n",
        "    'Name': [f'Student_{i}' for i in range(n_samples)],\n",
        "    'Age': np.random.randint(18, 30, n_samples).astype(float),\n",
        "    'Gender': np.random.choice(['Male', 'Female', 'M', 'F', 'male', 'female', None], n_samples),\n",
        "    'GPA': np.round(np.random.uniform(2.0, 4.0, n_samples), 2),\n",
        "    'Attendance': np.random.uniform(60, 100, n_samples),\n",
        "    'Study_Hours': np.random.uniform(5, 40, n_samples),\n",
        "    'Previous_Grade': np.random.choice(['A', 'B', 'C', 'D', 'F', None], n_samples),\n",
        "    'Scholarship': np.random.choice([True, False, 'Yes', 'No', None], n_samples)\n",
        "}\n",
        "\n",
        "# Introduce missing values\n",
        "df = pd.DataFrame(data)\n",
        "df.loc[np.random.choice(n_samples, 10), 'Age'] = np.nan\n",
        "df.loc[np.random.choice(n_samples, 15), 'GPA'] = np.nan\n",
        "df.loc[np.random.choice(n_samples, 8), 'Attendance'] = np.nan\n",
        "df.loc[np.random.choice(n_samples, 12), 'Study_Hours'] = np.nan\n",
        "\n",
        "# Introduce duplicates\n",
        "df = pd.concat([df, df.iloc[:5]], ignore_index=True)\n",
        "\n",
        "# Introduce outliers\n",
        "df.loc[np.random.choice(len(df), 3), 'Age'] = [150, -5, 200]\n",
        "df.loc[np.random.choice(len(df), 3), 'Study_Hours'] = [100, 150, -10]\n",
        "\n",
        "print(\"Raw Dataset Preview:\")\n",
        "print(df.head(10))\n",
        "print(f\"\\nDataset Shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDBZnw_Bx-f1"
      },
      "source": [
        "## 4.1 Initial Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy4SXSi5x-f1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# BASIC EXPLORATION\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA EXPLORATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìä Dataset Info:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqrcVWPSx-f2"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüìà Statistical Summary:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fNBWkSSx-f2"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüîç Missing Values:\")\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (missing / len(df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Missing %': missing_pct\n",
        "})\n",
        "print(missing_df[missing_df['Missing Count'] > 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHiY8jAmx-f2"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüîÑ Duplicate Rows:\", df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb7a0iyRx-f2"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüìù Unique Values in Categorical Columns:\")\n",
        "for col in ['Gender', 'Previous_Grade', 'Scholarship']:\n",
        "    print(f\"\\n{col}: {df[col].unique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HeWLr_4x-f2"
      },
      "source": [
        "## 4.2 Handling Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8JcfHxlx-gC"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# REMOVING DUPLICATES\n",
        "# ============================================\n",
        "\n",
        "print(f\"Before removing duplicates: {len(df)} rows\")\n",
        "\n",
        "# Find duplicates\n",
        "duplicates = df[df.duplicated(keep=False)]\n",
        "print(f\"\\nDuplicate rows found: {len(duplicates)}\")\n",
        "print(\"\\nDuplicate rows:\")\n",
        "print(duplicates.head())\n",
        "\n",
        "# Remove duplicates (keep first occurrence)\n",
        "df_clean = df.drop_duplicates(keep='first')\n",
        "print(f\"\\nAfter removing duplicates: {len(df_clean)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB7P6Z8ax-gC"
      },
      "source": [
        "## 4.3 Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIarYTWCx-gC"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# MISSING VALUE STRATEGIES\n",
        "# ============================================\n",
        "\n",
        "print(\"STRATEGIES FOR HANDLING MISSING VALUES:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Strategy 1: Drop rows with missing values\n",
        "print(\"\\n1Ô∏è‚É£ Strategy: Drop rows with missing values\")\n",
        "df_dropped = df_clean.dropna()\n",
        "print(f\"   Rows remaining: {len(df_dropped)} (lost {len(df_clean) - len(df_dropped)} rows)\")\n",
        "print(\"   ‚ö†Ô∏è Warning: May lose too much data!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MpS-h9Bx-gC"
      },
      "outputs": [],
      "source": [
        "# Strategy 2: Fill with specific values\n",
        "print(\"\\n2Ô∏è‚É£ Strategy: Fill numerical columns with mean/median\")\n",
        "df_filled = df_clean.copy()\n",
        "\n",
        "# For numerical columns - fill with median (more robust to outliers)\n",
        "numerical_cols = ['Age', 'GPA', 'Attendance', 'Study_Hours']\n",
        "for col in numerical_cols:\n",
        "    median_val = df_filled[col].median()\n",
        "    df_filled[col].fillna(median_val, inplace=True)\n",
        "    print(f\"   {col}: filled with median = {median_val:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf-PD2TVx-gC"
      },
      "outputs": [],
      "source": [
        "# Strategy 3: Fill categorical with mode\n",
        "print(\"\\n3Ô∏è‚É£ Strategy: Fill categorical columns with mode\")\n",
        "categorical_cols = ['Gender', 'Previous_Grade', 'Scholarship']\n",
        "for col in categorical_cols:\n",
        "    mode_val = df_filled[col].mode()[0] if not df_filled[col].mode().empty else 'Unknown'\n",
        "    df_filled[col].fillna(mode_val, inplace=True)\n",
        "    print(f\"   {col}: filled with mode = '{mode_val}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5ApzKAVx-gC"
      },
      "outputs": [],
      "source": [
        "# Verify no missing values remain\n",
        "print(\"\\n‚úÖ Missing values after filling:\")\n",
        "print(df_filled.isnull().sum().sum(), \"missing values remaining\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EILY4iZ0x-gC"
      },
      "source": [
        "## 4.4 Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BWMDtC5x-gC"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# DETECTING OUTLIERS\n",
        "# ============================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detect outliers using IQR method\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "print(\"OUTLIER DETECTION (IQR Method):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for col in ['Age', 'Study_Hours']:\n",
        "    outliers, lb, ub = detect_outliers_iqr(df_filled, col)\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Valid range: [{lb:.2f}, {ub:.2f}]\")\n",
        "    print(f\"  Outliers found: {len(outliers)}\")\n",
        "    if len(outliers) > 0:\n",
        "        print(f\"  Outlier values: {outliers[col].values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKO5_ZHSx-gC"
      },
      "outputs": [],
      "source": [
        "# Visualize outliers with boxplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "for idx, col in enumerate(['Age', 'Study_Hours']):\n",
        "    axes[idx].boxplot(df_filled[col].dropna())\n",
        "    axes[idx].set_title(f'{col} - Before Handling Outliers')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZri2PJjx-gC"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# HANDLING OUTLIERS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\nHANDLING OUTLIERS:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "df_no_outliers = df_filled.copy()\n",
        "\n",
        "def cap_outliers(data, column):\n",
        "    \"\"\"Cap outliers to the boundary values\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    data[column] = data[column].clip(lower=lower_bound, upper=upper_bound)\n",
        "    return data, lower_bound, upper_bound\n",
        "\n",
        "# Cap outliers in Age and Study_Hours\n",
        "for col in ['Age', 'Study_Hours']:\n",
        "    df_no_outliers, lb, ub = cap_outliers(df_no_outliers, col)\n",
        "    print(f\"{col}: capped to [{lb:.2f}, {ub:.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98AO-eXyx-gD"
      },
      "outputs": [],
      "source": [
        "# Visualize after handling outliers\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "for idx, col in enumerate(['Age', 'Study_Hours']):\n",
        "    axes[idx].boxplot(df_no_outliers[col].dropna())\n",
        "    axes[idx].set_title(f'{col} - After Handling Outliers')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaR68SuPx-gD"
      },
      "source": [
        "## 4.5 Data Standardization & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAd9XLGlx-gD"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STANDARDIZING CATEGORICAL VALUES\n",
        "# ============================================\n",
        "\n",
        "print(\"STANDARDIZING CATEGORICAL VALUES:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "df_standardized = df_no_outliers.copy()\n",
        "\n",
        "# Standardize Gender column\n",
        "print(f\"\\nGender - Before: {df_standardized['Gender'].unique()}\")\n",
        "\n",
        "gender_mapping = {\n",
        "    'Male': 'Male', 'M': 'Male', 'male': 'Male',\n",
        "    'Female': 'Female', 'F': 'Female', 'female': 'Female'\n",
        "}\n",
        "df_standardized['Gender'] = df_standardized['Gender'].replace(gender_mapping)\n",
        "\n",
        "print(f\"Gender - After: {df_standardized['Gender'].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hWTWs8Dx-gD"
      },
      "outputs": [],
      "source": [
        "# Standardize Scholarship column\n",
        "print(f\"\\nScholarship - Before: {df_standardized['Scholarship'].unique()}\")\n",
        "\n",
        "scholarship_mapping = {\n",
        "    True: 'Yes', 'Yes': 'Yes',\n",
        "    False: 'No', 'No': 'No'\n",
        "}\n",
        "df_standardized['Scholarship'] = df_standardized['Scholarship'].replace(scholarship_mapping)\n",
        "\n",
        "print(f\"Scholarship - After: {df_standardized['Scholarship'].unique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMgdureTx-gD"
      },
      "source": [
        "## 4.6 Encoding Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgKUSwi-x-gD"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ENCODING CATEGORICAL VARIABLES\n",
        "# ============================================\n",
        "\n",
        "print(\"ENCODING CATEGORICAL VARIABLES:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "df_encoded = df_standardized.copy()\n",
        "\n",
        "# Label Encoding for binary categorical variables\n",
        "print(\"\\n1Ô∏è‚É£ Label Encoding (for binary/ordinal variables):\")\n",
        "\n",
        "# Gender: Binary encoding\n",
        "df_encoded['Gender_Encoded'] = df_encoded['Gender'].map({'Male': 0, 'Female': 1})\n",
        "print(f\"   Gender: Male=0, Female=1\")\n",
        "\n",
        "# Previous Grade: Ordinal encoding\n",
        "grade_order = {'A': 4, 'B': 3, 'C': 2, 'D': 1, 'F': 0}\n",
        "df_encoded['Grade_Encoded'] = df_encoded['Previous_Grade'].map(grade_order)\n",
        "print(f\"   Previous_Grade: F=0, D=1, C=2, B=3, A=4\")\n",
        "\n",
        "# Scholarship: Binary encoding\n",
        "df_encoded['Scholarship_Encoded'] = df_encoded['Scholarship'].map({'No': 0, 'Yes': 1})\n",
        "print(f\"   Scholarship: No=0, Yes=1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxTZWe28x-gD"
      },
      "outputs": [],
      "source": [
        "# One-Hot Encoding (for nominal variables with no order)\n",
        "print(\"\\n2Ô∏è‚É£ One-Hot Encoding (for nominal variables):\")\n",
        "\n",
        "# One-hot encode Gender (demonstration)\n",
        "gender_dummies = pd.get_dummies(df_encoded['Gender'], prefix='Gender')\n",
        "print(\"   Gender one-hot encoded:\")\n",
        "print(gender_dummies.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfH-g_HYx-gD"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüìä Encoded DataFrame Preview:\")\n",
        "print(df_encoded[['Name', 'Gender', 'Gender_Encoded', 'Previous_Grade',\n",
        "                  'Grade_Encoded', 'Scholarship', 'Scholarship_Encoded']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av1gsqKjx-gD"
      },
      "source": [
        "## 4.7 Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5kUs5M_x-gD"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# FEATURE SCALING\n",
        "# ============================================\n",
        "\n",
        "print(\"FEATURE SCALING:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "numerical_features = ['Age', 'GPA', 'Attendance', 'Study_Hours']\n",
        "\n",
        "print(\"\\nOriginal values (before scaling):\")\n",
        "print(df_encoded[numerical_features].describe().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekxVByPZx-gD"
      },
      "outputs": [],
      "source": [
        "# Min-Max Scaling (Normalization) - scales to [0, 1]\n",
        "print(\"\\n1Ô∏è‚É£ Min-Max Scaling (Normalization) - scales to [0, 1]:\")\n",
        "\n",
        "df_scaled = df_encoded.copy()\n",
        "\n",
        "for col in numerical_features:\n",
        "    min_val = df_scaled[col].min()\n",
        "    max_val = df_scaled[col].max()\n",
        "    df_scaled[f'{col}_MinMax'] = (df_scaled[col] - min_val) / (max_val - min_val)\n",
        "\n",
        "print(df_scaled[[f'{col}_MinMax' for col in numerical_features]].describe().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bcr6atex-gD"
      },
      "outputs": [],
      "source": [
        "# Standard Scaling (Z-score normalization) - mean=0, std=1\n",
        "print(\"\\n2Ô∏è‚É£ Standard Scaling (Z-score) - mean=0, std=1:\")\n",
        "\n",
        "for col in numerical_features:\n",
        "    mean_val = df_scaled[col].mean()\n",
        "    std_val = df_scaled[col].std()\n",
        "    df_scaled[f'{col}_ZScore'] = (df_scaled[col] - mean_val) / std_val\n",
        "\n",
        "print(df_scaled[[f'{col}_ZScore' for col in numerical_features]].describe().round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPYo9uVUx-gD"
      },
      "source": [
        "## 4.8 Final Preprocessed Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgGTbh3Vx-gD"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# PREPARE FINAL DATASET FOR ML\n",
        "# ============================================\n",
        "\n",
        "# Select relevant columns for ML model\n",
        "ml_features = ['Age', 'GPA', 'Attendance', 'Study_Hours',\n",
        "               'Gender_Encoded', 'Grade_Encoded', 'Scholarship_Encoded']\n",
        "\n",
        "df_final = df_scaled[['Student_ID', 'Name'] + ml_features].copy()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL PREPROCESSED DATASET FOR ML\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nShape: {df_final.shape}\")\n",
        "print(f\"\\nColumns: {list(df_final.columns)}\")\n",
        "print(\"\\nPreview:\")\n",
        "print(df_final.head(10))\n",
        "print(\"\\nData Types:\")\n",
        "print(df_final.dtypes)\n",
        "print(\"\\n‚úÖ Dataset is ready for Machine Learning!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DGyesTMx-gD"
      },
      "source": [
        "## üí° Exercise 4.1: Comprehensive Preprocessing Challenge\n",
        "\n",
        "Apply all preprocessing techniques to the following dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukRfXLEZx-gD"
      },
      "outputs": [],
      "source": [
        "# Challenge Dataset\n",
        "np.random.seed(123)\n",
        "\n",
        "challenge_data = {\n",
        "    'Employee_ID': range(1, 51),\n",
        "    'Department': np.random.choice(['IT', 'HR', 'Sales', 'it', 'hr', 'SALES', None], 50),\n",
        "    'Salary': np.random.uniform(30000, 150000, 50),\n",
        "    'Experience_Years': np.random.uniform(0, 20, 50),\n",
        "    'Performance_Rating': np.random.choice(['Excellent', 'Good', 'Average', 'Poor', None], 50),\n",
        "    'Remote_Work': np.random.choice([True, False, 'Yes', 'No', '1', '0', None], 50)\n",
        "}\n",
        "\n",
        "challenge_df = pd.DataFrame(challenge_data)\n",
        "\n",
        "# Add some missing values\n",
        "challenge_df.loc[np.random.choice(50, 7), 'Salary'] = np.nan\n",
        "challenge_df.loc[np.random.choice(50, 5), 'Experience_Years'] = np.nan\n",
        "\n",
        "# Add some outliers\n",
        "challenge_df.loc[np.random.choice(50, 2), 'Salary'] = [500000, -10000]\n",
        "challenge_df.loc[np.random.choice(50, 2), 'Experience_Years'] = [50, -5]\n",
        "\n",
        "# Add duplicates\n",
        "challenge_df = pd.concat([challenge_df, challenge_df.iloc[:3]], ignore_index=True)\n",
        "\n",
        "print(\"Challenge Dataset:\")\n",
        "print(challenge_df.head(15))\n",
        "print(f\"\\nShape: {challenge_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGn57n2zx-gD"
      },
      "outputs": [],
      "source": [
        "# YOUR PREPROCESSING CODE HERE\n",
        "# Tasks:\n",
        "# 1. Explore the data (missing values, duplicates, unique values)\n",
        "# 2. Remove duplicates\n",
        "# 3. Handle missing values\n",
        "# 4. Handle outliers in Salary and Experience_Years\n",
        "# 5. Standardize Department and Remote_Work columns\n",
        "# 6. Encode categorical variables\n",
        "# 7. Scale numerical features\n",
        "\n",
        "# Step 1: Explore the data\n",
        "\n",
        "print(challenge_df.info())\n",
        "\n",
        "print(challenge_df.describe())\n",
        "\n",
        "missing_counts = challenge_df.isnull().sum()\n",
        "print(missing_counts[missing_counts > 0])\n",
        "\n",
        "print(f\"Number of duplicates: {challenge_df.duplicated().sum()}\")\n",
        "\n",
        "for col in ['Department', 'Performance_Rating', 'Remote_Work']:\n",
        "    print(f\"\\n{col}: {challenge_df[col].unique()}\")\n",
        "\n",
        "# Step 2: Remove duplicates\n",
        "print(f\"Before: {len(challenge_df)} rows\")\n",
        "challenge_df_clean = challenge_df.drop_duplicates()\n",
        "print(f\"After: {len(challenge_df_clean)} rows\")\n",
        "\n",
        "\n",
        "# Step 3: Handle missing values\n",
        "\n",
        "# Fill numerical columns with median\n",
        "challenge_df_clean['Salary'].fillna(challenge_df_clean['Salary'].median(), inplace=True)\n",
        "challenge_df_clean['Experience_Years'].fillna(challenge_df_clean['Experience_Years'].median(), inplace=True)\n",
        "\n",
        "# Fill categorical columns with mode\n",
        "challenge_df_clean['Department'].fillna(challenge_df_clean['Department'].mode()[0], inplace=True)\n",
        "challenge_df_clean['Performance_Rating'].fillna(challenge_df_clean['Performance_Rating'].mode()[0], inplace=True)\n",
        "challenge_df_clean['Remote_Work'].fillna('No', inplace=True)\n",
        "\n",
        "print(challenge_df_clean.isnull().sum())\n",
        "\n",
        "\n",
        "# Step 4: Handle outliers\n",
        "def cap_outliers(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    data[column] = data[column].clip(lower=lower_bound, upper=upper_bound)\n",
        "    return data, lower_bound, upper_bound\n",
        "\n",
        "  # (Cap outliers in Salary and Experience_Years)\n",
        "for col in ['Salary', 'Experience_Years']:\n",
        "    challenge_df_clean, lb, ub = cap_outliers(challenge_df_clean, col)\n",
        "    print(f\"{col}: capped to [{lb:.2f}, {ub:.2f}]\")\n",
        "\n",
        "\n",
        "# Step 5: Standardize categorical values\n",
        "\n",
        "# Standardize Department\n",
        "challenge_df_clean['Department'] = challenge_df_clean['Department'].replace({\n",
        "    'it': 'IT', 'hr': 'HR', 'SALES': 'Sales'\n",
        "})\n",
        "\n",
        "# Standardize Remote_Work\n",
        "challenge_df_clean['Remote_Work'] = challenge_df_clean['Remote_Work'].replace({\n",
        "    True: 'Yes', '1': 'Yes', 1: 'Yes',\n",
        "    False: 'No', '0': 'No', 0: 'No'\n",
        "})\n",
        "\n",
        "# Step 6: Encode categorical variables\n",
        "# Department\n",
        "challenge_df_clean['Department_Encoded'] = challenge_df_clean['Department'].map({\n",
        "    'IT': 0, 'HR': 1, 'Sales': 2\n",
        "})\n",
        "# Performance Rating\n",
        "challenge_df_clean['Performance_Encoded'] = challenge_df_clean['Performance_Rating'].map({\n",
        "    'Poor': 0, 'Average': 1, 'Good': 2, 'Excellent': 3\n",
        "})\n",
        "# Remote Work\n",
        "challenge_df_clean['Remote_Work_Encoded'] = challenge_df_clean['Remote_Work'].map({\n",
        "    'No': 0, 'Yes': 1\n",
        "})\n",
        "\n",
        "# Step 7: Scale numerical features\n",
        "\n",
        "numerical_features = ['Salary', 'Experience_Years']\n",
        "# Min-Max Scaling\n",
        "print(\"\\n Min-Max Scaling([0, 1]):\")\n",
        "for col in numerical_features:\n",
        "    min_val = challenge_df_clean[col].min()\n",
        "    max_val = challenge_df_clean[col].max()\n",
        "    challenge_df_clean[f'{col}_MinMax'] = (challenge_df_clean[col] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Standard Scaling\n",
        "print(\"\\nStandard Scaling (mean=0, std=1):\")\n",
        "for col in numerical_features:\n",
        "    mean_val = challenge_df_clean[col].mean()\n",
        "    std_val = challenge_df_clean[col].std()\n",
        "    challenge_df_clean[f'{col}_ZScore'] = (challenge_df_clean[col] - mean_val) / std_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g4q_Xmjx-gD"
      },
      "source": [
        "---\n",
        "\n",
        "# üìù Lab Summary\n",
        "\n",
        "## Key Takeaways:\n",
        "\n",
        "### 1. ML Project Lifecycle\n",
        "- 7 stages: Problem Definition ‚Üí Data Collection ‚Üí Data Preparation ‚Üí Feature Engineering ‚Üí Model Training ‚Üí Model Evaluation ‚Üí Deployment\n",
        "- Data preparation takes 60-80% of the time\n",
        "\n",
        "### 2. Environment Setup\n",
        "- **Google Colab**: Cloud-based, no setup, free GPU\n",
        "- **VSCode**: Local, customizable, requires setup\n",
        "\n",
        "### 3. NumPy Essentials\n",
        "- Array creation: `np.array()`, `np.zeros()`, `np.ones()`\n",
        "- Operations: element-wise, statistical functions\n",
        "- Indexing and slicing: similar to Python lists\n",
        "\n",
        "### 4. Pandas Essentials\n",
        "- Series (1D) and DataFrame (2D)\n",
        "- Selection: `[]`, `.loc[]`, `.iloc[]`\n",
        "- Filtering, grouping, aggregation\n",
        "\n",
        "### 5. Data Preprocessing Steps\n",
        "1. **Explore** data (info, describe, missing values)\n",
        "2. **Remove duplicates** (drop_duplicates)\n",
        "3. **Handle missing values** (drop, fill with mean/median/mode)\n",
        "4. **Handle outliers** (IQR method, capping)\n",
        "5. **Standardize** categorical values\n",
        "6. **Encode** categorical variables (label, one-hot)\n",
        "7. **Scale** numerical features (min-max, z-score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JIXPKJ2x-gD"
      },
      "source": [
        "---\n",
        "\n",
        "# üìö Lab Assignment\n",
        "\n",
        "## Instructions:\n",
        "\n",
        "1. Complete all exercises in this notebook (Exercises 1.1, 3.1, 3.2, and 4.1)\n",
        "2. For Exercise 4.1, document each step with comments explaining your choices\n",
        "3. Save your completed notebook with your name: `Lab1_YourName.ipynb`\n",
        "4. Create a GitHub repository dedicated to this course (e.g., CS4082-Machine-Learning-Labs).\n",
        "\n",
        "*   This repository will be used to store all lab work for the entire semester.\n",
        "\n",
        "*   Organize your work clearly (e.g., one folder per lab).\n",
        "5. Upload Lab 1 (Lab1_YourName.ipynb) to your GitHub repository in an appropriate folder (e.g., Lab1/).\n",
        "6. Submit the GitHub repository link via Blackboard before the deadline.\n",
        "\n",
        "## Grading Rubric:\n",
        "\n",
        "| Component | Points |\n",
        "|-----------|--------|\n",
        "| Exercise 1.1 (Problem Definition) | 10 |\n",
        "| Exercise 3.1 (NumPy) | 15 |\n",
        "| Exercise 3.2 (Pandas) | 15 |\n",
        "| Exercise 4.1 (Preprocessing Challenge) | 50 |\n",
        "| Code Quality & Comments | 10 |\n",
        "| **Total** | **100** |\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck! üçÄ**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}